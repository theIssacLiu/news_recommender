import numpy as np
import pandas as pd
from tqdm import tqdm
import pickle
import os
import random
import collections
import faiss
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Flatten, Concatenate, Layer
from tensorflow.keras import regularizers
import tensorflow as tf

# å®šä¹‰ç‰¹å¾åˆ—ç±»
class SparseFeat(object):
    def __init__(self, name, vocabulary_size, embedding_dim=8, use_hash=False, dtype='int32', embedding_name=None):
        self.name = name
        self.vocabulary_size = vocabulary_size
        self.embedding_dim = embedding_dim
        self.use_hash = use_hash
        self.dtype = dtype
        self.embedding_name = embedding_name if embedding_name else name

class VarLenSparseFeat(object):
    def __init__(self, sparsefeat, maxlen, combiner='mean', length_name=None):
        self.sparsefeat = sparsefeat
        self.maxlen = maxlen
        self.combiner = combiner
        self.length_name = length_name

# å®šä¹‰Sampled Softmax Loss
def sampledsoftmaxloss(y_true, y_pred):
    return y_pred

# YouTubeDNNæ¨¡å‹å®šä¹‰
class YoutubeDNN(Model):
    def __init__(self, user_feature_columns, item_feature_columns, user_dnn_hidden_units=(64, 32), 
                 dnn_activation='relu', dnn_use_bias=True, dnn_dropout=0, num_sampled=5, seed=1024, **kwargs):
        super(YoutubeDNN, self).__init__(**kwargs)
        
        self.user_feature_columns = user_feature_columns
        self.item_feature_columns = item_feature_columns
        self.hidden_units = user_dnn_hidden_units
        self.activation = dnn_activation
        self.use_bias = dnn_use_bias
        self.dropout = dnn_dropout
        self.num_sampled = num_sampled
        self.seed = seed
        
        # åˆ›å»ºç”¨æˆ·åµŒå…¥å­—å…¸
        self.user_embedding_dict = {}
        self.history_feature_list = []
        
        for feat in self.user_feature_columns:
            if isinstance(feat, SparseFeat):
                self.user_embedding_dict[feat.embedding_name] = Embedding(
                    input_dim=feat.vocabulary_size,
                    output_dim=feat.embedding_dim,
                    embeddings_initializer='random_normal',
                    embeddings_regularizer=regularizers.l2(1e-6),
                    name='emb_' + feat.name)
            elif isinstance(feat, VarLenSparseFeat):
                self.user_embedding_dict[feat.sparsefeat.embedding_name] = Embedding(
                    input_dim=feat.sparsefeat.vocabulary_size,
                    output_dim=feat.sparsefeat.embedding_dim,
                    embeddings_initializer='random_normal',
                    embeddings_regularizer=regularizers.l2(1e-6),
                    name='emb_' + feat.sparsefeat.name,
                    mask_zero=True)
                self.history_feature_list.append(feat.name)
        
        # åˆ›å»ºç‰©å“åµŒå…¥å­—å…¸
        self.item_embedding_dict = {}
        for feat in self.item_feature_columns:
            if isinstance(feat, SparseFeat):
                self.item_embedding_dict[feat.embedding_name] = Embedding(
                    input_dim=feat.vocabulary_size,
                    output_dim=feat.embedding_dim,
                    embeddings_initializer='random_normal',
                    embeddings_regularizer=regularizers.l2(1e-6),
                    name='emb_' + feat.name)
        
        # ç”¨æˆ·å¡”DNNå±‚
        self.user_dnn_layers = []
        for unit in self.hidden_units:
            self.user_dnn_layers.append(Dense(unit, activation=self.activation))
            self.user_dnn_layers.append(Dropout(self.dropout))
        
        # è¾“å‡ºå±‚
        self.output_layer = Dense(1, use_bias=False)
    
    def call(self, inputs, training=None):
        # ç”¨æˆ·ç‰¹å¾å¤„ç†
        user_embeddings = []
        sparse_embedding_list = []
        seq_embedding_list = []
        
        for feat in self.user_feature_columns:
            if isinstance(feat, SparseFeat):
                user_embedding = self.user_embedding_dict[feat.embedding_name](inputs[feat.name])
                user_embeddings.append(user_embedding)
            elif isinstance(feat, VarLenSparseFeat):
                seq_embedding = self.user_embedding_dict[feat.sparsefeat.embedding_name](inputs[feat.name])
                # è®¡ç®—åºåˆ—ç‰¹å¾çš„å¹³å‡å€¼
                hist_len = inputs[feat.length_name]
                mask = tf.sequence_mask(hist_len, feat.maxlen)
                mask = tf.cast(mask, tf.float32)
                mask = tf.expand_dims(mask, axis=-1)
                # é•¿åº¦å½’ä¸€åŒ–
                seq_embedding = seq_embedding * mask
                hist_len = tf.cast(hist_len, tf.float32)
                hist_len = tf.expand_dims(hist_len, axis=-1)
                seq_embedding = tf.reduce_sum(seq_embedding, axis=1) / tf.maximum(hist_len, 1.0)
                user_embeddings.append(seq_embedding)
        
        # ç‰©å“ç‰¹å¾å¤„ç†
        item_embeddings = []
        for feat in self.item_feature_columns:
            if isinstance(feat, SparseFeat):
                item_embedding = self.item_embedding_dict[feat.embedding_name](inputs[feat.name])
                item_embeddings.append(item_embedding)
        
        # å¤„ç†ç”¨æˆ·ç‰¹å¾å‘é‡
        user_embedding = tf.concat(user_embeddings, axis=-1)
        for layer in self.user_dnn_layers:
            user_embedding = layer(user_embedding)
        self.user_embedding = user_embedding
        
        # å¤„ç†ç‰©å“ç‰¹å¾å‘é‡
        item_embedding = tf.concat(item_embeddings, axis=-1)
        self.item_embedding = item_embedding
        
        # æ¨¡å‹è¾“å‡º
        output = tf.matmul(user_embedding, item_embedding, transpose_b=True)
        self.logits = output
        
        return output
    
    def summary(self):
        user_inputs = {feat.name: Input(shape=(1,) if isinstance(feat, SparseFeat) else (feat.maxlen,), name=feat.name) 
                      for feat in self.user_feature_columns}
        item_inputs = {feat.name: Input(shape=(1,), name=feat.name) for feat in self.item_feature_columns}
        
        all_inputs = {**user_inputs, **item_inputs}
        self.user_input = all_inputs
        self.item_input = item_inputs
        
        Model(inputs=all_inputs, outputs=self.call(all_inputs)).summary()

# è·å–åŒå¡”å¬å›æ—¶çš„è®­ç»ƒéªŒè¯æ•°æ®
# negsampleæŒ‡çš„æ˜¯é€šè¿‡æ»‘çª—æ„å»ºæ ·æœ¬çš„æ—¶å€™ï¼Œè´Ÿæ ·æœ¬çš„æ•°é‡
def gen_data_set(data, negsample=0):
    data.sort_values("click_timestamp", inplace=True)
    item_ids = data['click_article_id'].unique()

    train_set = []
    test_set = []
    for reviewerID, hist in tqdm(data.groupby('user_id')):
        pos_list = hist['click_article_id'].tolist()

        if negsample > 0:
            candidate_set = list(set(item_ids) - set(pos_list))  # ç”¨æˆ·æ²¡çœ‹è¿‡çš„æ–‡ç« é‡Œé¢é€‰æ‹©è´Ÿæ ·æœ¬
            neg_list = np.random.choice(candidate_set, size=len(pos_list) * negsample, replace=True)  # å¯¹äºæ¯ä¸ªæ­£æ ·æœ¬ï¼Œé€‰æ‹©nä¸ªè´Ÿæ ·æœ¬

        # é•¿åº¦åªæœ‰ä¸€ä¸ªçš„æ—¶å€™ï¼Œéœ€è¦æŠŠè¿™æ¡æ•°æ®ä¹Ÿæ”¾åˆ°è®­ç»ƒé›†ä¸­ï¼Œä¸ç„¶çš„è¯æœ€ç»ˆå­¦åˆ°çš„embeddingå°±ä¼šæœ‰ç¼ºå¤±
        if len(pos_list) == 1:
            train_set.append((reviewerID, [pos_list[0]], pos_list[0], 1, len(pos_list)))
            test_set.append((reviewerID, [pos_list[0]], pos_list[0], 1, len(pos_list)))

        # æ»‘çª—æ„é€ æ­£è´Ÿæ ·æœ¬
        for i in range(1, len(pos_list)):
            hist = pos_list[:i]

            if i != len(pos_list) - 1:
                train_set.append((reviewerID, hist[::-1], pos_list[i], 1,
                                  len(hist[::-1])))  # æ­£æ ·æœ¬ [user_id, his_item, pos_item, label, len(his_item)]
                for negi in range(negsample):
                    train_set.append((reviewerID, hist[::-1], neg_list[i * negsample + negi], 0,
                                      len(hist[::-1])))  # è´Ÿæ ·æœ¬ [user_id, his_item, neg_item, label, len(his_item)]
            else:
                # å°†æœ€é•¿çš„é‚£ä¸€ä¸ªåºåˆ—é•¿åº¦ä½œä¸ºæµ‹è¯•æ•°æ®
                test_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1])))

    random.shuffle(train_set)
    random.shuffle(test_set)

    return train_set, test_set


# å°†è¾“å…¥çš„æ•°æ®è¿›è¡Œpaddingï¼Œä½¿å¾—åºåˆ—ç‰¹å¾çš„é•¿åº¦éƒ½ä¸€è‡´
def gen_model_input(train_set, user_profile, seq_max_len):
    train_uid = np.array([line[0] for line in train_set])
    train_seq = [line[1] for line in train_set]
    train_iid = np.array([line[2] for line in train_set])
    train_label = np.array([line[3] for line in train_set])
    train_hist_len = np.array([line[4] for line in train_set])

    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)
    train_model_input = {"user_id": train_uid, "click_article_id": train_iid, "hist_article_id": train_seq_pad,
                         "hist_len": train_hist_len}

    return train_model_input, train_label


def youtubednn_u2i_dict(data, save_path="./cache/", topk=20, epochs=5, batch_size=256, validation_split=0.1):
    """
    è®­ç»ƒYouTubeDNNæ¨¡å‹å¹¶ç”Ÿæˆç”¨æˆ·-ç‰©å“å¬å›è¡¨
    
    Args:
        data: ç”¨æˆ·ç‚¹å‡»æ•°æ®ï¼ŒåŒ…å«user_idã€click_article_idå’Œclick_timestampåˆ—
        save_path: ç»“æœä¿å­˜è·¯å¾„
        topk: ä¸ºæ¯ä¸ªç”¨æˆ·å¬å›ç‰©å“çš„æ•°é‡
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹å¤§å°
        validation_split: éªŒè¯é›†æ¯”ä¾‹
        
    Returns:
        ç”¨æˆ·-ç‰©å“å¬å›è¡¨ï¼Œæ ¼å¼ä¸º{ç”¨æˆ·ID: [(ç‰©å“ID, å¾—åˆ†), ...]}
    """
    # ç¡®ä¿è·¯å¾„å­˜åœ¨
    os.makedirs(save_path, exist_ok=True)
    
    # å¦‚æœå­˜åœ¨ç¼“å­˜ç›´æ¥è¯»å–
    cache_path = os.path.join(save_path, 'youtube_u2i_dict.pkl')
    if os.path.exists(cache_path):
        print(f"[youtubednn_u2i_dict] âœ… ä½¿ç”¨ç¼“å­˜ï¼š{cache_path}")
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    
    print("[youtubednn_u2i_dict] ğŸš€ å¼€å§‹è®­ç»ƒYouTubeDNNæ¨¡å‹...")
    
    sparse_features = ["click_article_id", "user_id"]
    SEQ_LEN = 30  # ç”¨æˆ·ç‚¹å‡»åºåˆ—çš„é•¿åº¦ï¼ŒçŸ­çš„å¡«å……ï¼Œé•¿çš„æˆªæ–­

    user_profile_ = data[["user_id"]].drop_duplicates('user_id')
    item_profile_ = data[["click_article_id"]].drop_duplicates('click_article_id')

    # ç±»åˆ«ç¼–ç 
    features = ["click_article_id", "user_id"]
    feature_max_idx = {}

    for feature in features:
        lbe = LabelEncoder()
        data[feature] = lbe.fit_transform(data[feature])
        feature_max_idx[feature] = data[feature].max() + 1

    # æå–userå’Œitemçš„ç”»åƒï¼Œè¿™é‡Œå…·ä½“é€‰æ‹©å“ªäº›ç‰¹å¾è¿˜éœ€è¦è¿›ä¸€æ­¥çš„åˆ†æå’Œè€ƒè™‘
    user_profile = data[["user_id"]].drop_duplicates('user_id')
    item_profile = data[["click_article_id"]].drop_duplicates('click_article_id')

    user_index_2_rawid = dict(zip(user_profile['user_id'], user_profile_['user_id']))
    item_index_2_rawid = dict(zip(item_profile['click_article_id'], item_profile_['click_article_id']))

    # åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†
    # ç”±äºæ·±åº¦å­¦ä¹ éœ€è¦çš„æ•°æ®é‡é€šå¸¸éƒ½æ˜¯éå¸¸å¤§çš„ï¼Œæ‰€ä»¥ä¸ºäº†ä¿è¯å¬å›çš„æ•ˆæœï¼Œå¾€å¾€ä¼šé€šè¿‡æ»‘çª—çš„å½¢å¼æ‰©å……è®­ç»ƒæ ·æœ¬
    train_set, test_set = gen_data_set(data, negsample=4)  # æ·»åŠ è´Ÿæ ·æœ¬
    print(f"[youtubednn_u2i_dict] è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š{len(train_set)}ï¼Œæµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{len(test_set)}")
    
    # æ•´ç†è¾“å…¥æ•°æ®ï¼Œå…·ä½“çš„æ“ä½œå¯ä»¥çœ‹ä¸Šé¢çš„å‡½æ•°
    train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)
    test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)

    # ç¡®å®šEmbeddingçš„ç»´åº¦
    embedding_dim = 16

    # å°†æ•°æ®æ•´ç†æˆæ¨¡å‹å¯ä»¥ç›´æ¥è¾“å…¥çš„å½¢å¼
    user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),
                            VarLenSparseFeat(
                                SparseFeat('hist_article_id', feature_max_idx['click_article_id'], embedding_dim,
                                           embedding_name="click_article_id"), SEQ_LEN, 'mean', 'hist_len'), ]
    item_feature_columns = [SparseFeat('click_article_id', feature_max_idx['click_article_id'], embedding_dim)]

    # æ¨¡å‹çš„å®šä¹‰
    # num_sampled: è´Ÿé‡‡æ ·æ—¶çš„æ ·æœ¬æ•°é‡
    model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=5,
                       user_dnn_hidden_units=(64, embedding_dim))
    # æ¨¡å‹ç¼–è¯‘
    model.compile(optimizer="adam", loss=sampledsoftmaxloss)
    model.summary()

    print(f"[youtubednn_u2i_dict] å¼€å§‹è®­ç»ƒï¼Œepochs={epochs}, batch_size={batch_size}")
    # æ¨¡å‹è®­ç»ƒï¼Œè¿™é‡Œå¯ä»¥å®šä¹‰éªŒè¯é›†çš„æ¯”ä¾‹ï¼Œå¦‚æœè®¾ç½®ä¸º0çš„è¯å°±æ˜¯å…¨é‡æ•°æ®ç›´æ¥è¿›è¡Œè®­ç»ƒ
    history = model.fit(train_model_input, train_label, 
                       batch_size=batch_size, 
                       epochs=epochs, 
                       verbose=1, 
                       validation_split=validation_split)

    print("[youtubednn_u2i_dict] è®­ç»ƒå®Œæˆï¼Œæå–Embedding...")
    # è®­ç»ƒå®Œæ¨¡å‹ä¹‹å,æå–è®­ç»ƒçš„Embeddingï¼ŒåŒ…æ‹¬userç«¯å’Œitemç«¯
    test_user_model_input = test_model_input
    all_item_model_input = {"click_article_id": item_profile['click_article_id'].values}

    user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)
    item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)

    # ä¿å­˜å½“å‰çš„item_embedding å’Œ user_embedding æ’åºçš„æ—¶å€™å¯èƒ½èƒ½å¤Ÿç”¨åˆ°ï¼Œä½†æ˜¯éœ€è¦æ³¨æ„ä¿å­˜çš„æ—¶å€™éœ€è¦å’ŒåŸå§‹çš„idå¯¹åº”
    user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)
    item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)

    # embeddingä¿å­˜ä¹‹å‰å½’ä¸€åŒ–ä¸€ä¸‹
    user_embs = user_embs / np.linalg.norm(user_embs, axis=1, keepdims=True)
    item_embs = item_embs / np.linalg.norm(item_embs, axis=1, keepdims=True)

    # å°†Embeddingè½¬æ¢æˆå­—å…¸çš„å½¢å¼æ–¹ä¾¿æŸ¥è¯¢
    raw_user_id_emb_dict = {user_index_2_rawid[k]: \
                                v for k, v in zip(user_profile['user_id'], user_embs)}
    raw_item_id_emb_dict = {item_index_2_rawid[k]: \
                                v for k, v in zip(item_profile['click_article_id'], item_embs)}
    # å°†Embeddingä¿å­˜åˆ°æœ¬åœ°
    pickle.dump(raw_user_id_emb_dict, open(os.path.join(save_path, 'user_youtube_emb.pkl'), 'wb'))
    pickle.dump(raw_item_id_emb_dict, open(os.path.join(save_path, 'item_youtube_emb.pkl'), 'wb'))

    print("[youtubednn_u2i_dict] ä½¿ç”¨Faissè¿›è¡Œå‘é‡æ£€ç´¢...")
    # faissç´§é‚»æœç´¢ï¼Œé€šè¿‡user_embedding æœç´¢ä¸å…¶ç›¸ä¼¼æ€§æœ€é«˜çš„topkä¸ªitem
    index = faiss.IndexFlatIP(embedding_dim)
    # ä¸Šé¢å·²ç»è¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œè¿™é‡Œå¯ä»¥ä¸è¿›è¡Œå½’ä¸€åŒ–äº†
    #     faiss.normalize_L2(user_embs)
    #     faiss.normalize_L2(item_embs)
    index.add(item_embs)  # å°†itemå‘é‡æ„å»ºç´¢å¼•
    sim, idx = index.search(np.ascontiguousarray(user_embs), topk)  # é€šè¿‡userå»æŸ¥è¯¢æœ€ç›¸ä¼¼çš„topkä¸ªitem

    user_recall_items_dict = collections.defaultdict(dict)
    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(test_user_model_input['user_id'], sim, idx)):
        target_raw_id = user_index_2_rawid[target_idx]
        # ä»1å¼€å§‹æ˜¯ä¸ºäº†å»æ‰å•†å“æœ¬èº«, æ‰€ä»¥æœ€ç»ˆè·å¾—çš„ç›¸ä¼¼å•†å“åªæœ‰topk-1
        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):
            rele_raw_id = item_index_2_rawid[rele_idx]
            user_recall_items_dict[target_raw_id][rele_raw_id] = user_recall_items_dict.get(target_raw_id, {}) \
                                                                     .get(rele_raw_id, 0) + sim_value

    user_recall_items_dict = {k: sorted(v.items(), key=lambda x: x[1], reverse=True) for k, v in
                              user_recall_items_dict.items()}
    # å°†å¬å›çš„ç»“æœè¿›è¡Œæ’åº

    # ä¿å­˜å¬å›çš„ç»“æœ
    # è¿™é‡Œæ˜¯ç›´æ¥é€šè¿‡å‘é‡çš„æ–¹å¼å¾—åˆ°äº†å¬å›ç»“æœï¼Œç›¸æ¯”äºä¸Šé¢çš„å¬å›æ–¹æ³•ï¼Œä¸Šé¢çš„åªæ˜¯å¾—åˆ°äº†i2iåŠu2uçš„ç›¸ä¼¼æ€§çŸ©é˜µï¼Œè¿˜éœ€è¦è¿›è¡ŒååŒè¿‡æ»¤å¬å›æ‰èƒ½å¾—åˆ°å¬å›ç»“æœ
    # å¯ä»¥ç›´æ¥å¯¹è¿™ä¸ªå¬å›ç»“æœè¿›è¡Œè¯„ä¼°ï¼Œä¸ºäº†æ–¹ä¾¿å¯ä»¥ç»Ÿä¸€å†™ä¸€ä¸ªè¯„ä¼°å‡½æ•°å¯¹æ‰€æœ‰çš„å¬å›ç»“æœè¿›è¡Œè¯„ä¼°
    pickle.dump(user_recall_items_dict, open(os.path.join(save_path, 'youtube_u2i_dict.pkl'), 'wb'))
    print(f"[youtubednn_u2i_dict] âœ… YouTubeDNNå¬å›ç»“æœå·²ä¿å­˜è‡³ï¼š{os.path.join(save_path, 'youtube_u2i_dict.pkl')}")
    
    return user_recall_items_dict